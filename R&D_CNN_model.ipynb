{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "R&D - CNN model",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "51f4K3BiAtcT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24033170-6ba5-4e33-aa04-79f38e1085c0"
      },
      "source": [
        "import numpy\r\n",
        "import tensorflow as tf\r\n",
        "import os.path\r\n",
        "from keras.callbacks import ModelCheckpoint\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense\r\n",
        "from keras.layers import Conv1D, GlobalMaxPooling1D, MaxPooling1D\r\n",
        "from keras.layers.embeddings import Embedding\r\n",
        "from keras.preprocessing import sequence, text\r\n",
        "from keras.utils import np_utils\r\n",
        "from nltk.tokenize import word_tokenize\r\n",
        "import nltk\r\n",
        "from sklearn.metrics import confusion_matrix\r\n",
        "from sklearn.metrics import accuracy_score\r\n",
        "from sklearn.metrics import f1_score, average_precision_score\r\n",
        "nltk.download('punkt')\r\n",
        "\r\n",
        "# Set random seed for reproducibility\r\n",
        "seed = 7\r\n",
        "numpy.random.seed(seed)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "### Load the data, divide in train/dev/test, create vocabulary\r\n",
        "vocab = []\r\n",
        "X_total = []\r\n",
        "y_total = []\r\n",
        "\r\n",
        "with open(\"/steam_dataset.txt\", \"r\") as dataset:\r\n",
        "    for line in dataset:\r\n",
        "        X_total.append(line)\r\n",
        "        tokenize_word = word_tokenize(line)\r\n",
        "        for word in tokenize_word:\r\n",
        "            vocab.append(word)\r\n",
        "        \r\n",
        "X_train = X_total[:25000]\r\n",
        "X_dev = X_total[25000:25900]\r\n",
        "X_test = X_total[25900:]\r\n",
        "        \r\n",
        "with open(\"/steam_ratings.txt\", \"r\") as ratings:\r\n",
        "    for line in ratings:\r\n",
        "        y_total.append([int(line)])\r\n",
        "\r\n",
        "y_train = y_total[:25000]\r\n",
        "y_dev = y_total[25000:25900]\r\n",
        "y_test = y_total[25900:]\r\n",
        "\r\n",
        "unique_words = set(vocab)\r\n",
        "vocab = unique_words\r\n",
        "vocab_length = len(vocab) + int(len(vocab) / 10) # Added len for unknw words\r\n",
        "\r\n",
        "print(\"### !!! CONTROL LINE !!! ###\")\r\n",
        "print('Length of the sets: train', len(X_train), len(y_train))\r\n",
        "print('Length of the sets: dev', len(X_dev), len(y_dev))\r\n",
        "print('Length of the sets: test', len(X_test), len(y_test))\r\n",
        "print('Length of Vocabulary:', len(vocab))\r\n",
        "\r\n",
        "\r\n",
        "### Review Embedding\r\n",
        "X_train = [text.one_hot(rev, vocab_length) for rev in X_train]\r\n",
        "X_dev = [text.one_hot(rev, vocab_length) for rev in X_dev]\r\n",
        "X_test = [text.one_hot(rev, vocab_length) for rev in X_test]\r\n",
        "\r\n",
        "print(\"### !!! CONTROL LINE !!! ###\")\r\n",
        "print('Length of the Embedded sets and Example: train,', len(X_train), X_train[0])\r\n",
        "print('Length of the Embedded sets: dev', len(X_dev), X_dev[0])\r\n",
        "print('Length of the Embedded sets: test', len(X_test), X_test[0])\r\n",
        "\r\n",
        "# Length check and padding\r\n",
        "X_total = [text.one_hot(rev, vocab_length) for rev in X_total]\r\n",
        "word_count = lambda review: len(review)\r\n",
        "longest = max(X_total, key=word_count)\r\n",
        "longest_length = len(longest)\r\n",
        "\r\n",
        "X_train = sequence.pad_sequences(X_train, longest_length, padding='post')\r\n",
        "X_dev = sequence.pad_sequences(X_dev, longest_length, padding='post')\r\n",
        "X_test = sequence.pad_sequences(X_test, longest_length, padding='post')\r\n",
        "\r\n",
        "\r\n",
        "X_train = numpy.array(X_train)\r\n",
        "X_dev = numpy.array(X_dev)\r\n",
        "X_test = numpy.array(X_test)\r\n",
        "\r\n",
        "y_train = np_utils.to_categorical(y_train)\r\n",
        "y_dev = np_utils.to_categorical(y_dev)\r\n",
        "y_test = np_utils.to_categorical(y_test)\r\n",
        "y_train = numpy.array(y_train)\r\n",
        "y_dev = numpy.array(y_dev)\r\n",
        "y_test = numpy.array(y_test)\r\n",
        "\r\n",
        "print(\"### !!! CONTROL LINE !!! ###\")\r\n",
        "print('Length of the Pad-Embedded sets and Example: train,', len(X_train), X_train[0])\r\n",
        "print('Length of the Pad-Embedded sets: dev', len(X_dev), X_dev[0])\r\n",
        "print('Length of the Pad-Embedded sets: test', len(X_test), X_test[0])\r\n",
        "print('Length of the y: train,', len(y_train), y_train[0])\r\n",
        "print('Length of the y: dev', len(y_dev), y_dev[0])\r\n",
        "print('Length of the y', len(y_test), y_test[0])\r\n",
        "\r\n",
        "### Build the model\r\n",
        "\r\n",
        "print('Building the CNN model...')\r\n",
        "model = Sequential()\r\n",
        "model.add(Embedding(input_dim=vocab_length, output_dim=300, input_length=longest_length))\r\n",
        "model.add(Conv1D(activation=\"relu\", padding=\"valid\", filters=300, kernel_size=7))\r\n",
        "model.add(GlobalMaxPooling1D())\r\n",
        "model.add(Dense(600, activation='relu'))\r\n",
        "model.add(Dense(2, activation='softmax'))\r\n",
        "\r\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n",
        "\r\n",
        "print(model.summary())\r\n",
        "\r\n",
        "#checkpoint\r\n",
        "metric = 'val_accuracy'\r\n",
        "filepath=\"/weights-multi-best.hdf5\"\r\n",
        "checkpoint = ModelCheckpoint(filepath, monitor=metric, verbose=1, save_best_only=True,\r\n",
        "mode= 'max' )\r\n",
        "callbacks_list = [checkpoint]\r\n",
        "\r\n",
        "model.fit(X_train, y_train, batch_size=200, epochs=5, validation_data=(X_dev, y_dev), callbacks=callbacks_list)\r\n",
        "\r\n",
        "#load weights and recompile\r\n",
        "model.load_weights(filepath)\r\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n",
        "\r\n",
        "\r\n",
        "# evaluate the model\r\n",
        "print(\"-\" * 10)\r\n",
        "print(model.layers)\r\n",
        "print(\"-\" * 10)\r\n",
        "print(\"evaluation on dev\")\r\n",
        "scores = model.evaluate(X_dev, y_dev)\r\n",
        "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\r\n",
        "y_pred = model.predict(X_dev)\r\n",
        "y_pred = y_pred.argmax(1)\r\n",
        "y_dev = y_dev.argmax(1)\r\n",
        "print(\"Accuracy Rate by 'accuracy_score' is: %f\" % accuracy_score(y_dev, y_pred))\r\n",
        "print(\"Accuracy Rate by 'f1_score macro' is: %f\" % f1_score(y_dev, y_pred, average='macro'))\r\n",
        "\r\n",
        "print(\"evaluation on test\")\r\n",
        "scores = model.evaluate(X_test, y_test)\r\n",
        "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\r\n",
        "y_test_pred = model.predict(X_test)\r\n",
        "y_test_pred = y_test_pred.argmax(1)\r\n",
        "y_test = y_test.argmax(1)\r\n",
        "print(confusion_matrix(y_test, y_test_pred))\r\n",
        "print(\"Accuracy Rate by 'accuracy_score' is: %f\" % accuracy_score(y_test, y_test_pred))\r\n",
        "print(\"Accuracy Rate by 'f1_score macro' is: %f\" % f1_score(y_test, y_test_pred, average='macro'))\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "### !!! CONTROL LINE !!! ###\n",
            "Length of the sets: train 25000 25000\n",
            "Length of the sets: dev 900 900\n",
            "Length of the sets: test 2081 2081\n",
            "Length of Vocabulary: 49828\n",
            "### !!! CONTROL LINE !!! ###\n",
            "Length of the Embedded sets and Example: train, 25000 [49480, 25236, 30733, 33162, 49917, 5445, 17570, 47335, 29711, 39153, 39135, 33162, 29477, 35995, 52757, 49654, 49917, 26792, 50919, 5184, 40863, 5115, 33592, 33151, 7415, 42015, 29378, 51339, 49917, 13391, 27412, 3343, 36543, 33592, 3343, 7213, 21172, 26403, 17570, 35923, 50099, 14951, 49654, 54225, 44251, 51339, 53369, 35872, 22218, 39153, 19819, 4301, 3353, 48202, 19216, 24544, 15767, 30090, 35923, 4756, 23125, 44251, 36068, 38393, 6045, 7415, 48202, 16604, 51339, 18720, 33162, 5817, 161]\n",
            "Length of the Embedded sets: dev 900 [50679, 27661, 36186, 7213, 24276, 37877, 51339, 600, 48202, 40233, 33162, 2274, 37800, 42406, 24575]\n",
            "Length of the Embedded sets: test 2081 [52844, 40834, 44251, 49917, 1965, 19819, 46199, 39322, 46517, 37598, 1714, 9989, 49190, 4756, 33162, 16198, 16856, 29222, 29712, 37598, 39561, 25819, 19133, 35923, 14276, 52844, 21966, 37598, 22334, 33162, 49917, 34623, 7213, 38487, 48326, 29191, 5577, 27536, 40979, 1595, 34386, 6045, 19819]\n",
            "### !!! CONTROL LINE !!! ###\n",
            "Length of the Pad-Embedded sets and Example: train, 25000 [49480 25236 30733 ...     0     0     0]\n",
            "Length of the Pad-Embedded sets: dev 900 [50679 27661 36186 ...     0     0     0]\n",
            "Length of the Pad-Embedded sets: test 2081 [52844 40834 44251 ...     0     0     0]\n",
            "Length of the y: train, 25000 [1. 0.]\n",
            "Length of the y: dev 900 [1. 0.]\n",
            "Length of the y 2081 [0. 1.]\n",
            "Building the CNN model...\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 1507, 300)         16443000  \n",
            "_________________________________________________________________\n",
            "conv1d (Conv1D)              (None, 1501, 300)         630300    \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d (Global (None, 300)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 600)               180600    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 2)                 1202      \n",
            "=================================================================\n",
            "Total params: 17,255,102\n",
            "Trainable params: 17,255,102\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/5\n",
            "125/125 [==============================] - 2604s 21s/step - loss: 0.4401 - accuracy: 0.7858 - val_loss: 0.1412 - val_accuracy: 0.9522\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.95222, saving model to /weights-multi-best.hdf5\n",
            "Epoch 2/5\n",
            "125/125 [==============================] - 2591s 21s/step - loss: 0.1117 - accuracy: 0.9619 - val_loss: 0.1233 - val_accuracy: 0.9600\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.95222 to 0.96000, saving model to /weights-multi-best.hdf5\n",
            "Epoch 3/5\n",
            "125/125 [==============================] - 2601s 21s/step - loss: 0.0376 - accuracy: 0.9891 - val_loss: 0.1585 - val_accuracy: 0.9444\n",
            "\n",
            "Epoch 00003: val_accuracy did not improve from 0.96000\n",
            "Epoch 4/5\n",
            "125/125 [==============================] - 2598s 21s/step - loss: 0.0116 - accuracy: 0.9971 - val_loss: 0.1773 - val_accuracy: 0.9544\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.96000\n",
            "Epoch 5/5\n",
            "125/125 [==============================] - 2596s 21s/step - loss: 0.0054 - accuracy: 0.9987 - val_loss: 0.2237 - val_accuracy: 0.9433\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.96000\n",
            "----------\n",
            "[<tensorflow.python.keras.layers.embeddings.Embedding object at 0x7f88d1009588>, <tensorflow.python.keras.layers.convolutional.Conv1D object at 0x7f88d100b898>, <tensorflow.python.keras.layers.pooling.GlobalMaxPooling1D object at 0x7f88d100dd30>, <tensorflow.python.keras.layers.core.Dense object at 0x7f88d100df60>, <tensorflow.python.keras.layers.core.Dense object at 0x7f88d100fbe0>]\n",
            "----------\n",
            "evaluation on dev\n",
            "29/29 [==============================] - 28s 964ms/step - loss: 0.1733 - accuracy: 0.9501\n",
            "\n",
            "accuracy: 96.00%\n",
            "Accuracy Rate by 'accuracy_score' is: 0.960000\n",
            "Accuracy Rate by 'f1_score macro' is: 0.942552\n",
            "evaluation on test\n",
            "66/66 [==============================] - 65s 982ms/step - loss: 0.2416 - accuracy: 0.9159\n",
            "\n",
            "accuracy: 91.59%\n",
            "[[ 681   95]\n",
            " [  80 1225]]\n",
            "Accuracy Rate by 'accuracy_score' is: 0.915906\n",
            "Accuracy Rate by 'f1_score macro' is: 0.909738\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLIMD6gv05vn",
        "outputId": "076266a8-0bdb-407d-9640-7ae2864fccd5"
      },
      "source": [
        "import numpy\n",
        "import tensorflow as tf\n",
        "import os.path\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Conv1D, GlobalMaxPooling1D, MaxPooling1D\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.preprocessing import sequence, text\n",
        "from keras.utils import np_utils\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score, average_precision_score\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "seed = 7\n",
        "numpy.random.seed(seed)\n",
        "\n",
        "### Load the data, divide in train/dev/test, create vocabulary\n",
        "vocab = []\n",
        "X_total_S = []\n",
        "X_total_T = []\n",
        "y_total_S = []\n",
        "y_total_T = []\n",
        "\n",
        "with open(\"/steam_dataset.txt\", \"r\") as dataset:\n",
        "    for line in dataset:\n",
        "        X_total_S.append(line)\n",
        "        tokenize_word = word_tokenize(line)\n",
        "        for word in tokenize_word:\n",
        "            vocab.append(word)\n",
        "with open(\"/twitter_dataset.txt\", \"r\") as dataset:\n",
        "    for line in dataset:\n",
        "        X_total_T.append(line)\n",
        "        tokenize_word = word_tokenize(line)\n",
        "        for word in tokenize_word:\n",
        "            vocab.append(word)\n",
        "        \n",
        "X_train = X_total_S[:24600] + X_total_T[:11900]\n",
        "X_dev = X_total_S[24600:25500] + X_total_T[11900:12300]\n",
        "#set for other test -> X_test = X_total_S[25500:] #set for Steam test \n",
        "X_test = X_total_T[23900:]\n",
        "        \n",
        "with open(\"/steam_ratings.txt\", \"r\") as ratings:\n",
        "    for line in ratings:\n",
        "        y_total_S.append([int(line)])\n",
        "\n",
        "with open(\"/twitter_ratings.txt\", \"r\") as ratings:\n",
        "    for line in ratings:\n",
        "        y_total_T.append([int(line)])\n",
        "\n",
        "y_train = y_total_S[:24600] + y_total_T[:11900]\n",
        "y_dev = y_total_S[24600:25500] + y_total_T[11900:12300]\n",
        "#set for other test -> y_test = y_total_S[25500:] #set for Steam test\n",
        "y_test = y_total_T[23900:26051]\n",
        "\n",
        "unique_words = set(vocab)\n",
        "vocab = unique_words\n",
        "vocab_length = len(vocab) + int(len(vocab) / 10) # Added len for unknw words\n",
        "\n",
        "print(\"### !!! CONTROL LINE !!! ###\")\n",
        "print('Length of the sets: train', len(X_train), len(y_train))\n",
        "print('Length of the sets: dev', len(X_dev), len(y_dev))\n",
        "print('Length of the sets: test', len(X_test), len(y_test))\n",
        "print('Length of Vocabulary:', len(vocab))\n",
        "\n",
        "\n",
        "### Review Embedding\n",
        "X_train = [text.one_hot(rev, vocab_length) for rev in X_train]\n",
        "X_dev = [text.one_hot(rev, vocab_length) for rev in X_dev]\n",
        "X_test = [text.one_hot(rev, vocab_length) for rev in X_test]\n",
        "\n",
        "print(\"### !!! CONTROL LINE !!! ###\")\n",
        "print('Length of the Embedded sets and Example: train,', len(X_train), X_train[0])\n",
        "print('Length of the Embedded sets: dev', len(X_dev), X_dev[0])\n",
        "print('Length of the Embedded sets: test', len(X_test), X_test[0])\n",
        "\n",
        "# Length check and padding\n",
        "X_total = [text.one_hot(rev, vocab_length) for rev in X_total_S]\n",
        "word_count = lambda review: len(review)\n",
        "longest = max(X_total, key=word_count)\n",
        "longest_length = len(longest)\n",
        "\n",
        "X_train = sequence.pad_sequences(X_train, longest_length, padding='post')\n",
        "X_dev = sequence.pad_sequences(X_dev, longest_length, padding='post')\n",
        "X_test = sequence.pad_sequences(X_test, longest_length, padding='post')\n",
        "\n",
        "\n",
        "X_train = numpy.array(X_train)\n",
        "X_dev = numpy.array(X_dev)\n",
        "X_test = numpy.array(X_test)\n",
        "\n",
        "y_train = np_utils.to_categorical(y_train)\n",
        "y_dev = np_utils.to_categorical(y_dev)\n",
        "y_test = np_utils.to_categorical(y_test)\n",
        "y_train = numpy.array(y_train)\n",
        "y_dev = numpy.array(y_dev)\n",
        "y_test = numpy.array(y_test)\n",
        "\n",
        "print(\"### !!! CONTROL LINE !!! ###\")\n",
        "print('Length of the Pad-Embedded sets and Example: train,', len(X_train), X_train[0])\n",
        "print('Length of the Pad-Embedded sets: dev', len(X_dev), X_dev[0])\n",
        "print('Length of the Pad-Embedded sets: test', len(X_test), X_test[0])\n",
        "print('Length of the y: train,', len(y_train), y_train[0])\n",
        "print('Length of the y: dev', len(y_dev), y_dev[0])\n",
        "print('Length of the y', len(y_test), y_test[0])\n",
        "\n",
        "### Build the model\n",
        "\n",
        "print('Building the CNN model...')\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocab_length, output_dim=300, input_length=longest_length))\n",
        "model.add(Conv1D(activation=\"relu\", padding=\"valid\", filters=300, kernel_size=7))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "model.add(Dense(600, activation='relu'))\n",
        "model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "'''\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "print(model.summary())\n",
        "\n",
        "#checkpoint\n",
        "metric = 'val_accuracy'\n",
        "filepath=\"/St-weights-multi-best.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor=metric, verbose=1, save_best_only=True,\n",
        "mode= 'max' )\n",
        "callbacks_list = [checkpoint]\n",
        "\n",
        "model.fit(X_train, y_train, batch_size=200, epochs=5, validation_data=(X_dev, y_dev), callbacks=callbacks_list)\n",
        "'''\n",
        "filepath=\"/St-weights-multi-best.hdf5\"\n",
        "\n",
        "#load weights and recompile\n",
        "model.load_weights(filepath)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# evaluate the model\n",
        "print(\"-\" * 10)\n",
        "print(model.layers)\n",
        "print(\"-\" * 10)\n",
        "print(\"evaluation on dev\")\n",
        "scores = model.evaluate(X_dev, y_dev)\n",
        "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
        "y_pred = model.predict(X_dev)\n",
        "y_pred = y_pred.argmax(1)\n",
        "y_dev = y_dev.argmax(1)\n",
        "print(\"Accuracy Rate by 'accuracy_score' is: %f\" % accuracy_score(y_dev, y_pred))\n",
        "print(\"Accuracy Rate by 'f1_score macro' is: %f\" % f1_score(y_dev, y_pred, average='macro'))\n",
        "\n",
        "print(\"evaluation on test\")\n",
        "scores = model.evaluate(X_test, y_test)\n",
        "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
        "y_test_pred = model.predict(X_test)\n",
        "y_test_pred = y_test_pred.argmax(1)\n",
        "y_test = y_test.argmax(1)\n",
        "print(confusion_matrix(y_test, y_test_pred))\n",
        "print(\"Accuracy Rate by 'accuracy_score' is: %f\" % accuracy_score(y_test, y_test_pred))\n",
        "print(\"Accuracy Rate by 'f1_score macro' is: %f\" % f1_score(y_test, y_test_pred, average='macro'))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "### !!! CONTROL LINE !!! ###\n",
            "Length of the sets: train 36500 36500\n",
            "Length of the sets: dev 1300 1300\n",
            "Length of the sets: test 2151 2151\n",
            "Length of Vocabulary: 79317\n",
            "### !!! CONTROL LINE !!! ###\n",
            "Length of the Embedded sets and Example: train, 36500 [29445, 38094, 21918, 25974, 59796, 4240, 45653, 59891, 64825, 23748, 40069, 25974, 63994, 39398, 85135, 3869, 59796, 75928, 80202, 16197, 24949, 41724, 84906, 80529, 7253, 15085, 24006, 6129, 59796, 39934, 30384, 73130, 55648, 84906, 73130, 83055, 14105, 592, 45653, 48437, 9177, 84698, 3869, 25646, 33181, 6129, 24041, 22250, 44900, 23748, 66594, 22828, 33814, 74833, 10595, 33780, 45414, 50953, 48437, 41810, 86587, 33181, 69789, 9808, 26393, 7253, 74833, 57030, 6129, 38852, 25974, 37659, 3657]\n",
            "Length of the Embedded sets: dev 1300 [38951, 78224, 73996, 33814, 31132, 74574, 25974, 41267, 38210, 83055, 12040, 82869, 68837, 38066, 53412, 83055, 1251, 68165, 48904, 85258, 25974, 37659, 69927, 19286, 59796, 37517, 66594, 55139, 33181, 29486, 74833, 33714, 6129, 81269, 42516, 8587, 33828, 80348, 33181, 48818]\n",
            "Length of the Embedded sets: test 2151 [69924, 46208, 59796, 42279, 68837, 59796, 29671, 82802, 43190, 3869, 76385, 5800, 37780, 7021, 21743, 81417, 71396, 75170, 11563, 60471, 51862, 28921]\n",
            "### !!! CONTROL LINE !!! ###\n",
            "Length of the Pad-Embedded sets and Example: train, 36500 [29445 38094 21918 ...     0     0     0]\n",
            "Length of the Pad-Embedded sets: dev 1300 [38951 78224 73996 ...     0     0     0]\n",
            "Length of the Pad-Embedded sets: test 2151 [69924 46208 59796 ...     0     0     0]\n",
            "Length of the y: train, 36500 [1. 0.]\n",
            "Length of the y: dev 1300 [1. 0.]\n",
            "Length of the y 2151 [1. 0.]\n",
            "Building the CNN model...\n",
            "----------\n",
            "[<tensorflow.python.keras.layers.embeddings.Embedding object at 0x7f41c484aa20>, <tensorflow.python.keras.layers.convolutional.Conv1D object at 0x7f41c3edc9e8>, <tensorflow.python.keras.layers.pooling.GlobalMaxPooling1D object at 0x7f41bfae25c0>, <tensorflow.python.keras.layers.core.Dense object at 0x7f41bfae2128>, <tensorflow.python.keras.layers.core.Dense object at 0x7f41bdd95b38>]\n",
            "----------\n",
            "evaluation on dev\n",
            "41/41 [==============================] - 40s 976ms/step - loss: 0.8897 - accuracy: 0.5824\n",
            "\n",
            "accuracy: 66.15%\n",
            "Accuracy Rate by 'accuracy_score' is: 0.661538\n",
            "Accuracy Rate by 'f1_score macro' is: 0.541095\n",
            "evaluation on test\n",
            "68/68 [==============================] - 68s 1s/step - loss: 0.7461 - accuracy: 0.7057\n",
            "\n",
            "accuracy: 70.57%\n",
            "[[  43  436]\n",
            " [ 197 1475]]\n",
            "Accuracy Rate by 'accuracy_score' is: 0.705718\n",
            "Accuracy Rate by 'f1_score macro' is: 0.471471\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGlfDnVX2eXT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b81a9230-7926-44c4-9c22-aecefec99de0"
      },
      "source": [
        "import numpy\n",
        "import tensorflow as tf\n",
        "import os.path\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Conv1D, GlobalMaxPooling1D, MaxPooling1D\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.preprocessing import sequence, text\n",
        "from keras.utils import np_utils\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score, average_precision_score\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "seed = 7\n",
        "numpy.random.seed(seed)\n",
        "\n",
        "### Load the data, divide in train/dev/test, create vocabulary\n",
        "vocab = []\n",
        "X_total_S = []\n",
        "X_total_T = []\n",
        "y_total_S = []\n",
        "y_total_T = []\n",
        "\n",
        "with open(\"/steam_dataset.txt\", \"r\") as dataset:\n",
        "    for line in dataset:\n",
        "        X_total_S.append(line)\n",
        "        tokenize_word = word_tokenize(line)\n",
        "        for word in tokenize_word:\n",
        "            vocab.append(word)\n",
        "with open(\"/twitter_dataset.txt\", \"r\") as dataset:\n",
        "    for line in dataset:\n",
        "        X_total_T.append(line)\n",
        "        tokenize_word = word_tokenize(line)\n",
        "        for word in tokenize_word:\n",
        "            vocab.append(word)\n",
        "        \n",
        "X_train = X_total_S[:23100] + X_total_T[:23100]\n",
        "X_dev = X_total_S[23100:23900] + X_total_T[23100:23900]\n",
        "#set for other test -> X_test = X_total_S[23900:] #set for Steam test\n",
        "X_test = X_total_T[23900:] #set for test on Tweets\n",
        "        \n",
        "with open(\"/steam_ratings.txt\", \"r\") as ratings:\n",
        "    for line in ratings:\n",
        "        y_total_S.append([int(line)])\n",
        "\n",
        "with open(\"/twitter_ratings.txt\", \"r\") as ratings:\n",
        "    for line in ratings:\n",
        "        y_total_T.append([int(line)])\n",
        "\n",
        "y_train = y_total_S[:23100] + y_total_T[:23100]\n",
        "y_dev = y_total_S[23100:23900] + y_total_T[23100:23900]\n",
        "#set for other test -> y_test = y_total_S[23900:] #set for Steam test \n",
        "y_test = y_total_T[23900:26051] #set for test on Tweets\n",
        "\n",
        "unique_words = set(vocab)\n",
        "vocab = unique_words\n",
        "vocab_length = len(vocab) + int(len(vocab) / 10) # Added len for unknw words\n",
        "\n",
        "print(\"### !!! CONTROL LINE !!! ###\")\n",
        "print('Length of the sets: train', len(X_train), len(y_train))\n",
        "print('Length of the sets: dev', len(X_dev), len(y_dev))\n",
        "print('Length of the sets: test', len(X_test), len(y_test))\n",
        "print('Length of Vocabulary:', len(vocab))\n",
        "\n",
        "\n",
        "### Review Embedding\n",
        "X_train = [text.one_hot(rev, vocab_length) for rev in X_train]\n",
        "X_dev = [text.one_hot(rev, vocab_length) for rev in X_dev]\n",
        "X_test = [text.one_hot(rev, vocab_length) for rev in X_test]\n",
        "\n",
        "print(\"### !!! CONTROL LINE !!! ###\")\n",
        "print('Length of the Embedded sets and Example: train,', len(X_train), X_train[0])\n",
        "print('Length of the Embedded sets: dev', len(X_dev), X_dev[0])\n",
        "print('Length of the Embedded sets: test', len(X_test), X_test[0])\n",
        "\n",
        "# Length check and padding\n",
        "X_total = [text.one_hot(rev, vocab_length) for rev in X_total_S]\n",
        "word_count = lambda review: len(review)\n",
        "longest = max(X_total, key=word_count)\n",
        "longest_length = len(longest)\n",
        "\n",
        "X_train = sequence.pad_sequences(X_train, longest_length, padding='post')\n",
        "X_dev = sequence.pad_sequences(X_dev, longest_length, padding='post')\n",
        "X_test = sequence.pad_sequences(X_test, longest_length, padding='post')\n",
        "\n",
        "\n",
        "X_train = numpy.array(X_train)\n",
        "X_dev = numpy.array(X_dev)\n",
        "X_test = numpy.array(X_test)\n",
        "\n",
        "y_train = np_utils.to_categorical(y_train)\n",
        "y_dev = np_utils.to_categorical(y_dev)\n",
        "y_test = np_utils.to_categorical(y_test)\n",
        "y_train = numpy.array(y_train)\n",
        "y_dev = numpy.array(y_dev)\n",
        "y_test = numpy.array(y_test)\n",
        "\n",
        "print(\"### !!! CONTROL LINE !!! ###\")\n",
        "print('Length of the Pad-Embedded sets and Example: train,', len(X_train), X_train[0])\n",
        "print('Length of the Pad-Embedded sets: dev', len(X_dev), X_dev[0])\n",
        "print('Length of the Pad-Embedded sets: test', len(X_test), X_test[0])\n",
        "print('Length of the y: train,', len(y_train), y_train[0])\n",
        "print('Length of the y: dev', len(y_dev), y_dev[0])\n",
        "print('Length of the y', len(y_test), y_test[0])\n",
        "\n",
        "### Build the model\n",
        "\n",
        "print('Building the CNN model...')\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocab_length, output_dim=300, input_length=longest_length))\n",
        "model.add(Conv1D(activation=\"relu\", padding=\"valid\", filters=300, kernel_size=7))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "model.add(Dense(600, activation='relu'))\n",
        "model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "print(model.summary())\n",
        "'''\n",
        "#checkpoint\n",
        "metric = 'val_accuracy'\n",
        "filepath=\"/ST-weights-multi-best.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor=metric, verbose=1, save_best_only=True,\n",
        "mode= 'max' )\n",
        "callbacks_list = [checkpoint]\n",
        "\n",
        "# Since 5 epochs are not enough for this model, up to 8\n",
        "model.fit(X_train, y_train, batch_size=200, epochs=8, validation_data=(X_dev, y_dev), callbacks=callbacks_list)\n",
        "'''\n",
        "filepath=\"/ST-weights-multi-best.hdf5\"\n",
        "\n",
        "#load weights and recompile\n",
        "model.load_weights(filepath)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# evaluate the model\n",
        "print(\"-\" * 10)\n",
        "print(model.layers)\n",
        "print(\"-\" * 10)\n",
        "print(\"evaluation on dev\")\n",
        "scores = model.evaluate(X_dev, y_dev)\n",
        "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
        "y_pred = model.predict(X_dev)\n",
        "y_pred = y_pred.argmax(1)\n",
        "y_dev = y_dev.argmax(1)\n",
        "print(\"Accuracy Rate by 'accuracy_score' is: %f\" % accuracy_score(y_dev, y_pred))\n",
        "print(\"Accuracy Rate by 'f1_score macro' is: %f\" % f1_score(y_dev, y_pred, average='macro'))\n",
        "\n",
        "print(\"evaluation on test\")\n",
        "scores = model.evaluate(X_test, y_test)\n",
        "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
        "y_test_pred = model.predict(X_test)\n",
        "y_test_pred = y_test_pred.argmax(1)\n",
        "y_test = y_test.argmax(1)\n",
        "print(confusion_matrix(y_test, y_test_pred))\n",
        "print(\"Accuracy Rate by 'accuracy_score' is: %f\" % accuracy_score(y_test, y_test_pred))\n",
        "print(\"Accuracy Rate by 'f1_score macro' is: %f\" % f1_score(y_test, y_test_pred, average='macro'))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "### !!! CONTROL LINE !!! ###\n",
            "Length of the sets: train 46200 46200\n",
            "Length of the sets: dev 1600 1600\n",
            "Length of the sets: test 2151 2151\n",
            "Length of Vocabulary: 79317\n",
            "### !!! CONTROL LINE !!! ###\n",
            "Length of the Embedded sets and Example: train, 46200 [29445, 38094, 21918, 25974, 59796, 4240, 45653, 59891, 64825, 23748, 40069, 25974, 63994, 39398, 85135, 3869, 59796, 75928, 80202, 16197, 24949, 41724, 84906, 80529, 7253, 15085, 24006, 6129, 59796, 39934, 30384, 73130, 55648, 84906, 73130, 83055, 14105, 592, 45653, 48437, 9177, 84698, 3869, 25646, 33181, 6129, 24041, 22250, 44900, 23748, 66594, 22828, 33814, 74833, 10595, 33780, 45414, 50953, 48437, 41810, 86587, 33181, 69789, 9808, 26393, 7253, 74833, 57030, 6129, 38852, 25974, 37659, 3657]\n",
            "Length of the Embedded sets: dev 1600 [48437, 3009, 2643, 10827, 77474, 31704, 50111, 79212]\n",
            "Length of the Embedded sets: test 2151 [69924, 46208, 59796, 42279, 68837, 59796, 29671, 82802, 43190, 3869, 76385, 5800, 37780, 7021, 21743, 81417, 71396, 75170, 11563, 60471, 51862, 28921]\n",
            "### !!! CONTROL LINE !!! ###\n",
            "Length of the Pad-Embedded sets and Example: train, 46200 [29445 38094 21918 ...     0     0     0]\n",
            "Length of the Pad-Embedded sets: dev 1600 [48437  3009  2643 ...     0     0     0]\n",
            "Length of the Pad-Embedded sets: test 2151 [69924 46208 59796 ...     0     0     0]\n",
            "Length of the y: train, 46200 [1. 0.]\n",
            "Length of the y: dev 1600 [1. 0.]\n",
            "Length of the y 2151 [1. 0.]\n",
            "Building the CNN model...\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 1507, 300)         26174400  \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 1501, 300)         630300    \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d_1 (Glob (None, 300)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 600)               180600    \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 2)                 1202      \n",
            "=================================================================\n",
            "Total params: 26,986,502\n",
            "Trainable params: 26,986,502\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "----------\n",
            "[<tensorflow.python.keras.layers.embeddings.Embedding object at 0x7f41c4ba6198>, <tensorflow.python.keras.layers.convolutional.Conv1D object at 0x7f41c4ba61d0>, <tensorflow.python.keras.layers.pooling.GlobalMaxPooling1D object at 0x7f41c484ae80>, <tensorflow.python.keras.layers.core.Dense object at 0x7f41c484ab00>, <tensorflow.python.keras.layers.core.Dense object at 0x7f41c45b6cf8>]\n",
            "----------\n",
            "evaluation on dev\n",
            "50/50 [==============================] - 50s 990ms/step - loss: 0.3176 - accuracy: 0.9382\n",
            "\n",
            "accuracy: 95.13%\n",
            "Accuracy Rate by 'accuracy_score' is: 0.951250\n",
            "Accuracy Rate by 'f1_score macro' is: 0.947568\n",
            "evaluation on test\n",
            "68/68 [==============================] - 67s 977ms/step - loss: 0.0134 - accuracy: 0.9991\n",
            "\n",
            "accuracy: 99.91%\n",
            "[[ 478    1]\n",
            " [   1 1671]]\n",
            "Accuracy Rate by 'accuracy_score' is: 0.999070\n",
            "Accuracy Rate by 'f1_score macro' is: 0.998657\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}